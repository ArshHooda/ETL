# ğŸŒ Earthquake ETL Pipeline on Azure

Welcome to the **Earthquake Data Pipeline** project! This solution implements a scalable, end-to-end ETL pipeline on **Microsoft Azure** to process live earthquake data from the **USGS Earthquake API** using a **medallion architecture** pattern.

---

## ğŸš€ Overview

This pipeline fetches, transforms, and stores seismic event data in Azure Data Lake using three structured layers:

ğŸ”¹ **Bronze** â†’ Raw Data  
ğŸ”¸ **Silver** â†’ Cleaned Data  
ğŸ”¶ **Gold** â†’ Enriched Data

Technologies used include **Azure Databricks**, **PySpark**, and **reverse_geocoder** for geolocation tagging.

---

## ğŸ—ï¸ Pipeline Architecture


USGS API ğŸŒ

â†“

Bronze Layer ğŸ“¦ (Raw JSON)

â†“

Silver Layer ğŸ§¹ (Cleaned Parquet)

â†“

Gold Layer âœ¨ (Enriched CSV + Metadata)



---

## ğŸ¥‡ Medallion Layer Details

### ğŸ¥‰ Bronze (Raw Layer)
- ğŸ“¥ Extracts raw earthquake JSON from the [USGS API](https://earthquake.usgs.gov/fdsnws/event/1/).
- ğŸ’¾ Stores data in **ADLS Gen2** under `bronze/` container.

### ğŸ¥ˆ Silver (Cleaned Layer)
- ğŸ§½ Transforms raw JSON into structured schema.
- ğŸ› ï¸ Cleans missing values, converts timestamps.
- ğŸ’¾ Stores as **Parquet** in `silver/` container.

### ğŸ¥‡ Gold (Enriched Layer)
- ğŸ—ºï¸ Adds geolocation using `reverse_geocoder`.
- ğŸ“Š Classifies earthquakes by significance (`Low`, `Moderate`, `High`).
- ğŸ’¾ Stores final **CSV** data in `gold/` container.

---

## ğŸ§° Technologies Used

| Tool / Service        | Purpose                        |
|-----------------------|--------------------------------|
| âš¡ Databricks         | Orchestration & Spark compute |
| ğŸ§Š ADLS Gen2          | Scalable storage for all layers |
| ğŸŒ USGS API           | Earthquake data source        |
| ğŸ PySpark + Python   | ETL & UDF logic               |
| ğŸ“ reverse_geocoder   | Country code enrichment       |

---

## âš™ï¸ Setup & Execution

### âœ… 1. Prerequisites
- âœ”ï¸ Azure Databricks workspace
- âœ”ï¸ ADLS Gen2 with containers:
  - `bronze` â†’ Raw JSON
  - `silver` â†’ Cleaned Parquet
  - `gold` â†’ Enriched CSV
- âœ”ï¸ Databricks cluster with:
  - PySpark
  - `reverse_geocoder` Python lib installed

---

### â–¶ï¸ 2. Running the Pipeline

#### ğŸŸ¤ Bronze Notebook
- ğŸ“… Input: `start_date`, `end_date` (YYYY-MM-DD)
- ğŸ“‚ Output: Raw JSON files â†’ `bronze/`

#### âšª Silver Notebook
- ğŸ”„ Input: Reads from Bronze layer
- ğŸ“‚ Output: Cleaned Parquet â†’ `silver/`

#### ğŸŸ¡ Gold Notebook
- ğŸ”„ Input: Reads from Silver layer
- ğŸŒ Enrichment: Adds location + tags magnitude
- ğŸ“‚ Output: Enriched CSV â†’ `gold/`

---

## ğŸ“ˆ Example Use Case
Build real-time dashboards in **Power BI** or **Synapse Analytics** using the Gold layer to:
- ğŸ—ºï¸ Map earthquake hotspots
- ğŸ“‰ Track seismic trends
- ğŸš¨ Set up alerts for high-risk zones

